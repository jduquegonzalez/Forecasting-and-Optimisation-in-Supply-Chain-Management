{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding New Features to the Training Set\n",
    "(With comments and codes from the Nicolas Vandepu's book \"Data Science for Supply Chain Forecasting\") ->>https://supchains.com/books/#book1\n",
    "\n",
    "For many businesses, historical demand is not the only—or main—factor that drives future sales. Other internal and external factors drive the demand as well. You might sell more or less depending on the weather, the GDP growth, unemployment rate, loan rates, and so on. These external factors (external, as The demand can also be driven by company decisions: price changes, promotions, marketing budget, or another product’s sales. As these factors result from business decisions, we will call them internal factors.\n",
    "\n",
    "We will show both historical demand and historical GDP growth side by side. This will allow the tool to understand that, historically, the sales could have been high or low due to favorable or unfavorable GDP growth.\n",
    "\n",
    "In practice, when creating our training dataset, we’ll add the GDP growth next to the historical demand in X_train. See the implementation of X_train and Y_train in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Transformation for Time Series Analysis\n",
    "Let's defines a function called import_data that automates the process of importing and transforming car sales data from a CSV file into a pandas DataFrame formatted for time series analysis. The CSV file, located at the provided URL, contains monthly car sales data organized by the car make and model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Quantity                                                          \\\n",
      "Period        2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08   \n",
      "Make                                                                            \n",
      "Alfa Romeo         16       9      21      20      17      21      14      12   \n",
      "Aston Martin        0       0       1       0       4       3       3       0   \n",
      "Audi              599     498     682     556     630     498     562     590   \n",
      "BMW               352     335     365     360     431     477     403     348   \n",
      "Bentley             0       0       0       0       0       1       0       0   \n",
      "\n",
      "                              ...                                          \\\n",
      "Period       2007-09 2007-10  ... 2016-04 2016-05 2016-06 2016-07 2016-08   \n",
      "Make                          ...                                           \n",
      "Alfa Romeo        15      10  ...       3       1       2       1       6   \n",
      "Aston Martin       0       0  ...       0       0       1       0       0   \n",
      "Audi             393     554  ...     685     540     551     687     794   \n",
      "BMW              271     562  ...    1052     832     808     636    1031   \n",
      "Bentley            0       0  ...       0       0       1       1       1   \n",
      "\n",
      "                                                      \n",
      "Period       2016-09 2016-10 2016-11 2016-12 2017-01  \n",
      "Make                                                  \n",
      "Alfa Romeo        15       3       4       3       6  \n",
      "Aston Martin       0       0       0       0       0  \n",
      "Audi             688     603     645     827     565  \n",
      "BMW             1193    1096    1663     866    1540  \n",
      "Bentley            0       0       0       0       0  \n",
      "\n",
      "[5 rows x 121 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the import_data function\n",
    "def import_data():\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Period'] = data['Year'].astype(str) + '-' + data['Month'].astype(str).str.zfill(2)\n",
    "    df = pd.pivot_table(data=data, values=['Quantity'], index='Make', columns='Period', aggfunc='sum', fill_value=0)\n",
    "    return df\n",
    "\n",
    "# URL of the CSV file\n",
    "file_path = \"https://supchains.com/wp-content/uploads/2021/07/norway_new_car_sales_by_make1.csv\"\n",
    "\n",
    "# Create the DataFrame using the import_data function\n",
    "df = import_data()\n",
    "\n",
    "# Now 'df' contains the data from the provided URL in the desired format.\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving and Structuring Economic Data for Time Series Analysis\n",
    "Now we are going to fetch economic data using an API and then structure it into a pandas DataFrame for time series analysis. The specific data retrieved is the volume of the gross domestic product (GDP) across various years, from 2006 to 2017.\n",
    "Here's what the script does in detail:\n",
    "\n",
    "- It imports the necessary requests module for making HTTP requests and pandas for data manipulation.\n",
    "- It specifies the URL for the API endpoint that will provide the economic data.\n",
    "- It constructs a JSON payload that defines the parameters for the API request. This includes specifying the macroeconomic indicator for GDP (\"bnpb.nr23_9\"), the content code for the volume (\"Volum\"), and the range of years of interest.\n",
    "- It sends a POST request to the API with the payload and parses the JSON response.\n",
    "- It extracts the GDP values and corresponding years from the response data.\n",
    "- It creates a pandas DataFrame, X_exo, with two columns: 'Year' and 'GDP', where 'Year' is converted to a datetime format.\n",
    "- It sets the 'Year' column as the index of the DataFrame and formats the index to represent periods (e.g., '2006-01', '2006-02', etc.).\n",
    "- Finally, it prints the resulting DataFrame, which is now ready for further time series analysis tasks, such as forecasting or trend analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         GDP\n",
      "Year        \n",
      "2006-01  2.5\n",
      "2007-01  2.9\n",
      "2008-01  0.5\n",
      "2009-01 -1.9\n",
      "2010-01  0.8\n",
      "2011-01  1.1\n",
      "2012-01  2.7\n",
      "2013-01  1.0\n",
      "2014-01  2.0\n",
      "2015-01  1.9\n",
      "2016-01  1.2\n",
      "2017-01  2.5\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# URL for the API request\n",
    "url = \"https://data.ssb.no/api/v0/en/table/09189/\"\n",
    "\n",
    "# JSON payload for the request\n",
    "payload = {\n",
    "    \"query\": [\n",
    "        {\n",
    "            \"code\": \"Makrost\",\n",
    "            \"selection\": {\n",
    "                \"filter\": \"item\",\n",
    "                \"values\": [\"bnpb.nr23_9\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"code\": \"ContentsCode\",\n",
    "            \"selection\": {\n",
    "                \"filter\": \"item\",\n",
    "                \"values\": [\"Volum\"]\n",
    "            }\n",
    "        },\n",
    "        {\n",
    "            \"code\": \"Tid\",\n",
    "            \"selection\": {\n",
    "                \"filter\": \"item\",\n",
    "                \"values\": [str(year) for year in range(2006, 2018)]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"response\": {\n",
    "        \"format\": \"json-stat2\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Making the API request\n",
    "response = requests.post(url, json=payload)\n",
    "data = response.json()\n",
    "\n",
    "# Extracting the values and years\n",
    "values = data['value']\n",
    "years = [year for year in data['dimension']['Tid']['category']['label']]\n",
    "\n",
    "# Creating a DataFrame\n",
    "X_exo = pd.DataFrame({\n",
    "    'Year': years,\n",
    "    'GDP': values\n",
    "})\n",
    "\n",
    "# Setting the year as index and formatting it to '%Y-%m'\n",
    "X_exo['Year'] = pd.to_datetime(X_exo['Year'], format='%Y')\n",
    "X_exo.set_index('Year', inplace=True)\n",
    "X_exo.index = X_exo.index.to_period('M')\n",
    "\n",
    "# Displaying the DataFrame\n",
    "print(X_exo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Machine Learning Data Preparation for Time Series Forecasting with Exogenous Variables\n",
    "The function datasets_exo will be used to prepare datasets from a pandas DataFrame for time series forecasting in machine learning models that will include the exogenous variable X_exo. The function is tailored to work with time series data where the goal is to predict future values based on past observations and additional external factors.\n",
    "The key operations performed by the function include:\n",
    "\n",
    "- **Data Conversion:** The input DataFrame df, which contains time series data, is converted into a NumPy array D for more efficient numerical operations.\n",
    "\n",
    "- **Exogenous Variables Preparation:** The array of exogenous variables X_exo is created by repeating the given exogenous data across all time series rows.\n",
    "\n",
    "- **Month Extraction:** The function assumes the DataFrame columns include month information in their names. It extracts this information, turning it into a numerical array that indicates the time within the series.\n",
    "\n",
    "- **Training Set Creation:** The function constructs the training set by looping through the time series data and creating \"windows\" of observations of length x_len for the input features, along with an additional y_len for the target variable.\n",
    "\n",
    "- **Test Set Creation:** Depending on the test_loops parameter, the function can also create a test set by separating out the last few sequences of data. If no test set is requested, it prepares a dataset for future forecasting, using dummy values as placeholders for the unknown future values.\n",
    "\n",
    "- **Scikit-Learn Compatibility:** The target arrays are reshaped to be compatible with scikit-learn's expected input format, especially when the target sequence length y_len is one, requiring a 1D array.\n",
    "\n",
    "This practice will allow us to evaluate the model's performance on unseen data and to simulate making future predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def datasets_exo(df, X_exo, x_len=12, y_len=1, test_loops=12):\n",
    "    # Convert DataFrame to numpy array\n",
    "    D = df.values\n",
    "    rows, periods = D.shape  # Get the number of rows and columns from the DataFrame\n",
    "\n",
    "    # Prepare the exogenous variables by repeating them for each row\n",
    "    X_exo = np.repeat(np.reshape(X_exo, [1, -1]), rows, axis=0)\n",
    "\n",
    "    # Prepare the month variables by repeating them for each row\n",
    "    X_months = np.repeat(np.reshape([int(col[-2:]) for col in df.columns], [1, -1]), rows, axis=0)\n",
    "\n",
    "    # Training set creation\n",
    "    loops = periods + 1 - x_len - y_len  # Determine the number of loops for creating the training set\n",
    "    train = []  # Initialize the training set list\n",
    "\n",
    "    for col in range(loops):\n",
    "        m = X_months[:, col:col+x_len].reshape(-1, 1)  # month\n",
    "        exo = X_exo[:, col:col+x_len].reshape(-1, 1)  # exogenous data\n",
    "        d = D[:, col:col+x_len+y_len]  # target data\n",
    "        train.append(np.hstack([m, exo, d]))  # Combine the month, exogenous data, and target data\n",
    "\n",
    "    train = np.vstack(train)  # Stack the training data vertically\n",
    "    X_train, Y_train = np.split(train, [-y_len], axis=1)  # Split the training data into features and target\n",
    "\n",
    "    # Test set creation\n",
    "    if test_loops > 0:\n",
    "        X_train, X_test = np.split(X_train, [-rows*test_loops], axis=0)\n",
    "        Y_train, Y_test = np.split(Y_train, [-rows*test_loops], axis=0)\n",
    "    else:  # No test set: X_test is used to generate the future forecast\n",
    "        X_test = np.hstack([m[:, -1].reshape(-1, 1), X_exo[:, -x_len:], D[:, -x_len:]])\n",
    "        Y_test = np.full((X_test.shape[0], y_len), np.nan)  # Dummy values\n",
    "\n",
    "    # Formatting required for scikit-learn\n",
    "    if y_len == 1:\n",
    "        Y_train = Y_train.ravel()\n",
    "        Y_test = Y_test.ravel()\n",
    "\n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, you can simply use this function to generate the new train and test arrays. These can then be used in the various models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
