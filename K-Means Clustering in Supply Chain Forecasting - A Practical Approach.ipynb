{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccbd9480",
   "metadata": {},
   "source": [
    "## K-means Clustering\n",
    "**(With extracts from the Nicolas Vandepu's book \"Data Science for Supply Chain Forecasting\")**\n",
    "\n",
    "K-means clustering is a widely utilized unsupervised learning technique in data science for classifying unlabeled data into groups based on their features, not pre-defined categories. The method involves partitioning data points into 'k' distinct non-overlapping subgroups or clusters. The number 'k' represents the number of clusters to be created, which is a parameter defined by the user. The main goal of K-means is to minimize the sum of the squared distances between the data points and their respective cluster centroid, which is the center of mass for each cluster. This algorithm is typically the first choice for practitioners to get an understanding of the dataset's structure due to its simplicity and effectiveness in revealing patterns and regularities within the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07a5e578",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Quantity                                                          \\\n",
      "Period        2007-01 2007-02 2007-03 2007-04 2007-05 2007-06 2007-07 2007-08   \n",
      "Make                                                                            \n",
      "Alfa Romeo         16       9      21      20      17      21      14      12   \n",
      "Aston Martin        0       0       1       0       4       3       3       0   \n",
      "Audi              599     498     682     556     630     498     562     590   \n",
      "BMW               352     335     365     360     431     477     403     348   \n",
      "Bentley             0       0       0       0       0       1       0       0   \n",
      "\n",
      "                              ...                                          \\\n",
      "Period       2007-09 2007-10  ... 2016-04 2016-05 2016-06 2016-07 2016-08   \n",
      "Make                          ...                                           \n",
      "Alfa Romeo        15      10  ...       3       1       2       1       6   \n",
      "Aston Martin       0       0  ...       0       0       1       0       0   \n",
      "Audi             393     554  ...     685     540     551     687     794   \n",
      "BMW              271     562  ...    1052     832     808     636    1031   \n",
      "Bentley            0       0  ...       0       0       1       1       1   \n",
      "\n",
      "                                                      \n",
      "Period       2016-09 2016-10 2016-11 2016-12 2017-01  \n",
      "Make                                                  \n",
      "Alfa Romeo        15       3       4       3       6  \n",
      "Aston Martin       0       0       0       0       0  \n",
      "Audi             688     603     645     827     565  \n",
      "BMW             1193    1096    1663     866    1540  \n",
      "Bentley            0       0       0       0       0  \n",
      "\n",
      "[5 rows x 121 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define the import_data function\n",
    "def import_data():\n",
    "    data = pd.read_csv(file_path)\n",
    "    data['Period'] = data['Year'].astype(str) + '-' + data['Month'].astype(str).str.zfill(2)\n",
    "    df = pd.pivot_table(data=data, values=['Quantity'], index='Make', columns='Period', aggfunc='sum', fill_value=0)\n",
    "    return df\n",
    "\n",
    "# URL of the CSV file\n",
    "file_path = \"https://supchains.com/wp-content/uploads/2021/07/norway_new_car_sales_by_make1.csv\"\n",
    "\n",
    "# Create the DataFrame using the import_data function\n",
    "df = import_data()\n",
    "\n",
    "# Now 'df' contains the data from the provided URL in the desired format.\n",
    "\n",
    "# Print the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Looking for Meaningful Centers\n",
    " \n",
    "We just learned a compelling and automated way to label an entire dataset, thanks to machine learning. But how can we apply this to our historical demand? \n",
    "The question you should ask yourself is: what are the features I want to categorize my products on? Depending on each dataset, you might prefer different approaches. Here are some ideas: \n",
    "               \n",
    "- **Volume:**\n",
    "We can obviously start categorizing products based on their sales volume. Most likely, this categorization won’t provide so much added value, as you could just do a good old Pareto Classification to get a similar result. \n",
    "- **Additive Seasonality:**\n",
    "If we cluster the products based on their additive seasonality factors, we will cluster them based on their average volume and their seasonality. You might then end up with groups containing just a few products. \n",
    "- **Multiplicative Seasonality:** \n",
    "If we take the multiplicative seasonal factors, our products will then be categorized only based on their seasonal behavior—irrespective of their absolute size. This sounds much better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling\n",
    "The K-means algorithm is very sensitive to extreme values (that, by definition, are far away from any other values) so that we will have to normalize all the seasonal factors. It means that each product’s seasonal factors will be reduced to have a mean of 0 and a range of 1.\n",
    "\n",
    "K-means is very sensitive to scaling; always remember to scale your dataset before applying this technique. As we did here, it is always a best practice to visualize your results to check that they are meaningful. Another good check is to count the number of products in each cluster: if you obtain some clusters with only a few items, that might be a clue that the clusters are not meaningful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Compute the (multiplicative) seasonal factorsLet’s \n",
    "In order to do so, we will create a function seasonal_factors() that will return the seasonal factors based on a historical demand dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seasonal_factors(df, slen):\n",
    "    # Create a new DataFrame 's' with the same index as 'df' to store the seasonal factors\n",
    "    s = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Loop over the range of seasonal length 'slen' to calculate mean values\n",
    "    # for each season across all columns\n",
    "    for i in range(slen):\n",
    "        # Calculate the mean of every 'slen' elements in each column and assign it\n",
    "        # to a new column in DataFrame 's'. This mean is a seasonal factor.\n",
    "        # 'i::slen' is a slicing technique that starts at index 'i' and skips 'slen' elements at a time.\n",
    "        s[i+1] = df.iloc[:, i::slen].mean(axis=1)\n",
    "    \n",
    "    # Normalize each column in 's' by dividing by the mean of the column to get the relative seasonal factors\n",
    "    # Then, replace any missing values resulted from division by zero with 0.\n",
    "    s = s.divide(s.mean(axis=1), axis=0).fillna(0)\n",
    "    \n",
    "    # Return the DataFrame containing seasonal factors for each season\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a scaler() function \n",
    "It will return the seasonality factors scaled with a range of 1 and a mean of 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(s):\n",
    "    # Calculate the mean of each row\n",
    "    mean = s.mean(axis=1)\n",
    "    \n",
    "    # Find the maximum value in each row\n",
    "    maxi = s.max(axis=1)\n",
    "    \n",
    "    # Find the minimum value in each row\n",
    "    mini = s.min(axis=1)\n",
    "    \n",
    "    # Subtract the mean from each element in the DataFrame (broadcasting along rows)\n",
    "    s = s.subtract(mean, axis=0)\n",
    "    \n",
    "    # Perform element-wise division of the DataFrame by the range (max - min)\n",
    "    # Broadcasting along rows. If the range is zero, 'fillna(0)' handles division by zero by replacing NaNs with 0.\n",
    "    s = s.divide(maxi-mini, axis=0).fillna(0)\n",
    "    \n",
    "    # Return the scaled DataFrame\n",
    "    return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use both our functions to populate scaled seasonal factors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'index'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\jonat\\OneDrive - Global Banking School\\supchains.com\\Extreme Gradient Boosting Optimization (single and multiple output) copy.ipynb Cell 10\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df \u001b[39m=\u001b[39m import_data\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m s \u001b[39m=\u001b[39m seasonal_factors(df,slen\u001b[39m=\u001b[39;49m\u001b[39m12\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m s\u001b[39m=\u001b[39m scaler(s)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(s\u001b[39m.\u001b[39mhead())\n",
      "\u001b[1;32mc:\\Users\\jonat\\OneDrive - Global Banking School\\supchains.com\\Extreme Gradient Boosting Optimization (single and multiple output) copy.ipynb Cell 10\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mseasonal_factors\u001b[39m(df, slen):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m     \u001b[39m# Create a new DataFrame 's' with the same index as 'df' to store the seasonal factors\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     s \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mDataFrame(index\u001b[39m=\u001b[39mdf\u001b[39m.\u001b[39;49mindex)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m# Loop over the range of seasonal length 'slen' to calculate mean values\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     \u001b[39m# for each season across all columns\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(slen):\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m         \u001b[39m# Calculate the mean of every 'slen' elements in each column and assign it\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m         \u001b[39m# to a new column in DataFrame 's'. This mean is a seasonal factor.\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/jonat/OneDrive%20-%20Global%20Banking%20School/supchains.com/Extreme%20Gradient%20Boosting%20Optimization%20%28single%20and%20multiple%20output%29%20copy.ipynb#X45sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m         \u001b[39m# 'i::slen' is a slicing technique that starts at index 'i' and skips 'slen' elements at a time.\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'function' object has no attribute 'index'"
     ]
    }
   ],
   "source": [
    "df = import_data\n",
    "s = seasonal_factors(df,slen=12)\n",
    "s= scaler(s)\n",
    "print(s.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "725f4ac3",
   "metadata": {},
   "source": [
    "### Training and Test Sets Creation\n",
    "Now that we have our dataset with the proper formatting, we can create a function datasets() to populate a training and a test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7d785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define the datasets function with x_len as an argument\n",
    "def datasets(df, x_len=12, y_len=1, test_loops=12):\n",
    "    D = df.values\n",
    "    rows, periods = D.shape\n",
    "    \n",
    "    # Training set creation\n",
    "    loops = periods + 1 - x_len - y_len\n",
    "    train = []\n",
    "    for col in range(loops):\n",
    "        train.append(D[:, col:col + x_len + y_len])\n",
    "    train = np.vstack(train)\n",
    "    X_train, Y_train = np.split(train, [-y_len], axis=1)\n",
    "    \n",
    "    # Test set creation\n",
    "    if test_loops > 0:\n",
    "        X_train, X_test = np.split(X_train, [-rows * test_loops], axis=0)\n",
    "        Y_train, Y_test = np.split(Y_train, [-rows * test_loops], axis=0)\n",
    "    else:\n",
    "        X_test = D[:, -x_len:]\n",
    "        Y_test = np.full((X_test.shape[0], y_len), np.nan)\n",
    "    \n",
    "    # Formatting required for scikit-learn\n",
    "    if y_len == 1:\n",
    "        Y_train = Y_train.ravel()\n",
    "        Y_test = Y_test.ravel()\n",
    "        \n",
    "    return X_train, Y_train, X_test, Y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fafc95",
   "metadata": {},
   "source": [
    "### Call our new function datasets(df) as well as import_data()\n",
    "We can now easily call our new function datasets(df) as well as import_data(). We obtain the datasets we need to feed our machine learning algorithm (X_train and Y_train) and the datasets we need to test it (X_test and Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239cfdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "df = import_data()\n",
    "\n",
    "# Create training and test sets using the datasets function\n",
    "X_train, Y_train, X_test, Y_test = datasets(df, x_len=12, y_len=1, test_loops=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b1e1c1",
   "metadata": {},
   "source": [
    "### Analyzing Feature Importance with XGBoost in Machine Learning\n",
    "Machine learning algorithms often operate on large datasets with numerous features, making it essential to understand the importance of each feature in the model's predictions. XGBoost, an efficient and scalable machine learning algorithm, provides a built-in method to assess feature importance. In this code snippet, we utilize XGBoost to create a regression model and analyze the importance of features using its plotting capabilities. Let's delve into the organized version of the code to gain insights into how feature importance is visualized and interpreted in the context of machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ad9071",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBRegressor\n",
    "\n",
    "# Initializing XGBoost regression model with specified parameters\n",
    "XGB = XGBRegressor(n_jobs=-1, max_depth=10, n_estimators=100, learning_rate=0.2)\n",
    "\n",
    "# Fitting the XGBoost model with training data\n",
    "XGB.fit(X_train, Y_train)\n",
    "\n",
    "# Assigning feature names to the booster object for visualization\n",
    "XGB.get_booster().feature_names = [f'M{x-12}' for x in range(12)]\n",
    "\n",
    "# Plotting feature importance using XGBoost's plot_importance function\n",
    "xgb.plot_importance(XGB, importance_type='total_gain', show_values=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb86ee9",
   "metadata": {},
   "source": [
    "### Evaluation and Early Stopping\n",
    "When an XGBoost model is trained on a dataset, you can measure—after each iteration—its accuracy against an evaluation set.\n",
    "\n",
    "**Evaluation Set**\n",
    "An evaluation set is a set of data that is left aside from the training set to be used as a monitoring dataset during the training. A validation set (random subset of the training set) or a holdout set (last period of the training set) can be used as an evaluation set.\n",
    "\n",
    "If you want to optimize your model’s parameters, the best practice is of course to run a cross-validation random search. But rather than trying different models with a different number of trees, you could grow your model indefinitely, and stop it when there is no more improvement on the evaluation set (for some consecutive iterations).\n",
    "\n",
    "In practice, once the model does not see an accuracy improvement on the evaluation test for a determined number of extra trees, XGBoost will revert to the last iteration that brought extra accuracy to the evaluation set. With this technique, you get rid of the burden of number-of-trees optimization, and you are sure to grow your model up to the right level. \n",
    "               This early stopping technique will help us avoid overfitting our model to the training set, and at the same time, reduce training time. One stone, two birds. The early stopping technique is a very useful capability of XGBoost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "x_train, x_val, y_train, y_val = train_test_split(X_train, Y_train, test_size=0.15)\n",
    "\n",
    "# Setting up the XGBoost regressor with specified hyperparameters\n",
    "# Including the `eval_metric` and `early_stopping_rounds` during initialization\n",
    "XGB = XGBRegressor(\n",
    "    n_jobs=-1, \n",
    "    max_depth=10, \n",
    "    n_estimators=1000, \n",
    "    learning_rate=0.01, \n",
    "    early_stopping_rounds=100,\n",
    "    eval_metric='mae'\n",
    ")\n",
    "\n",
    "# Fitting the model on the training data\n",
    "# Note: No need to specify `early_stopping_rounds` and `eval_metric` here\n",
    "XGB.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    verbose=False, \n",
    "    eval_set=[(x_val, y_val)]\n",
    ")\n",
    "\n",
    "# Printing the best iteration number\n",
    "print(f'Best iteration: {XGB.best_iteration}')\n",
    "\n",
    "# Printing the best score\n",
    "print(f'Best score: {XGB.best_score}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206f5f0e",
   "metadata": {},
   "source": [
    "#### Using holdout dataset as the evaluation dataset\n",
    "Instead of a validation set, you can also use a holdout dataset as the evaluation dataset. This can perform better on some datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def datasets_holdout(df, x_len=12, y_len=1, test_loops=12, holdout_loops=0):\n",
    "    D = df.values\n",
    "    rows, periods = D.shape\n",
    "    \n",
    "    # Training set creation\n",
    "    train_loops = periods + 1 - x_len - y_len - test_loops\n",
    "    train = []\n",
    "    for col in range(train_loops):\n",
    "        train.append(D[:, col:col + x_len + y_len])\n",
    "    train = np.vstack(train)\n",
    "    X_train, Y_train = np.split(train, [-y_len], axis=1)\n",
    "    \n",
    "    # Holdout set creation\n",
    "    if holdout_loops > 0:\n",
    "        X_train, X_holdout = np.split(X_train, [-rows * holdout_loops], axis=0)\n",
    "        Y_train, Y_holdout = np.split(Y_train, [-rows * holdout_loops], axis=0)\n",
    "    else:\n",
    "        X_holdout, Y_holdout = np.array([]), np.array([])\n",
    "    \n",
    "    # Test set creation\n",
    "    if test_loops > 0:\n",
    "        X_train, X_test = np.split(X_train, [-rows * test_loops], axis=0)\n",
    "        Y_train, Y_test = np.split(Y_train, [-rows * test_loops], axis=0)\n",
    "    else:\n",
    "        # No test set: X_test is used to generate the future forecast\n",
    "        X_test = D[:, -x_len:]\n",
    "        Y_test = np.full((X_test.shape[0], y_len), np.nan)  # Dummy value\n",
    "    \n",
    "    # Formatting required for scikit-learn\n",
    "    if y_len == 1:\n",
    "        Y_train = Y_train.ravel()\n",
    "        Y_test = Y_test.ravel()\n",
    "        Y_holdout = Y_holdout.ravel()\n",
    "    \n",
    "    return X_train, Y_train, X_holdout, Y_holdout, X_test, Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating training, holdout, and test sets using a custom function\n",
    "X_train, Y_train, X_holdout, Y_holdout, X_test, Y_test = datasets_holdout(df, x_len=12, y_len=1, test_loops=12, holdout_loops=12)\n",
    "\n",
    "# Initializing and training XGBoost regression model\n",
    "XGB = XGBRegressor(n_jobs=-1, max_depth=10, n_estimators=2000, learning_rate=0.01)\n",
    "XGB = XGB.fit(X_train, Y_train, verbose=False, eval_metric='mae')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Predicting values for holdout and test sets\n",
    "predictions_holdout = XGB.predict(X_holdout)\n",
    "predictions_test = XGB.predict(X_test)\n",
    "\n",
    "# Calculating MAE for holdout and test sets\n",
    "mae_holdout = mean_absolute_error(Y_holdout, predictions_holdout)\n",
    "mae_test = mean_absolute_error(Y_test, predictions_test)\n",
    "\n",
    "# Creating a DataFrame with the results\n",
    "results_data = {\n",
    "    'Dataset': ['Holdout', 'Test'],\n",
    "    'Mean Absolute Error (MAE)': [mae_holdout, mae_test]\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(results_data)\n",
    "\n",
    "# Displaying the table\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important note:** It's important to compare the MAE to some baseline or to the range of the target variable to understand the magnitude of these errors fully. Additionally, if these datasets are different in size, distribution, or feature space, the MAE values may not be directly comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4730b5f",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning Grid for Gradient Boosting Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f7d5e6",
   "metadata": {},
   "source": [
    "#### Optimization\n",
    "Now that we have defined the parameter space we want to test, we can continue and perform a cross-validation random search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Initialize the XGBRegressor with default parameters\n",
    "XGB = XGBRegressor()\n",
    "\n",
    "# Define the hyperparameters to be tuned\n",
    "params = {\n",
    "    'max_depth': [5, 6, 7, 8, 10, 11],\n",
    "    'learning_rate': [0.005, 0.01, 0.025, 0.05, 0.1, 0.15],\n",
    "    'colsample_bynode': [0.5, 0.6, 0.7, 0.8, 0.9, 1.0],  # max_features\n",
    "    'colsample_bylevel': [0.8, 0.9, 1.0],  # max_features per level\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0],  # max_features\n",
    "    'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7],  # max_samples\n",
    "    'min_child_weight': [5, 10, 15, 20, 25],  # min_samples_leaf\n",
    "    'reg_alpha': [1, 5, 10, 20, 50],\n",
    "    'reg_lambda': [0.01, 0.05, 0.1, 0.5, 1.1],    'n_estimators': [1000]\n",
    "}\n",
    "\n",
    "# Define RandomizedSearchCV\n",
    "XGB_cv = RandomizedSearchCV(\n",
    "    XGB, \n",
    "    params, \n",
    "    cv=5, \n",
    "    n_jobs=-1, \n",
    "    verbose=1, \n",
    "    n_iter=1000, \n",
    "    scoring='neg_mean_absolute_error'\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "XGB_cv.fit(X_train, Y_train)\n",
    "\n",
    "# Print the best parameters\n",
    "print('Tuned XGBoost Parameters: ', XGB_cv.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** In this setup, the eval_set parameter is not directly set in the fit_params since it's not straightforward to use with cross-validation (like RandomizedSearchCV), as each fold will have a different validation set.\n",
    "\n",
    "**Pro-Tip - Double Search**\n",
    "\n",
    "If you don't feel confident about setting the various parameter ranges to be tested, do not hesitate to run two optimizations one after the other- the first one with wide parameter ranges; then, a second one performed in more detail around the first optimal values found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now train XGBoostRegressor with RandomizedSearchCV-Optimized Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomizedSearchCV object with best parameters determined\n",
    "best_params = XGB_cv.best_params_\n",
    "\n",
    "# Initialize XGBRegressor with the best parameters\n",
    "XGB = XGBRegressor(n_jobs=-1, **best_params)\n",
    "\n",
    "# Assuming fit_params is defined as before, without early stopping rounds and eval_metric\n",
    "fit_params = {\n",
    "    'verbose': False\n",
    "}\n",
    "\n",
    "# Fit the model with the training data\n",
    "XGB.fit(x_train, y_train, **fit_params)\n",
    "\n",
    "# Print the best iteration and score if available\n",
    "# These attributes are only available if early stopping is used\n",
    "if hasattr(XGB.get_booster(), 'best_iteration'):\n",
    "    print(f'Best iteration: {XGB.get_booster().best_iteration}')\n",
    "if hasattr(XGB.get_booster(), 'best_score'):\n",
    "    print(f'Best score: {XGB.get_booster().best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on the training and test sets\n",
    "y_train_pred = XGB.predict(X_train)\n",
    "y_test_pred = XGB.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the kpi_ML function\n",
    "def kpi_ML(Y_train, Y_train_pred, Y_test, Y_test_pred, name=\"\"):\n",
    "    df = pd.DataFrame(columns=['MAE', 'RMSE', 'Bias'], index=['Train', 'Test'])\n",
    "    df.index.name = name\n",
    "\n",
    "    df.loc['Train', 'MAE'] = 100 * np.mean(np.abs(Y_train - Y_train_pred)) / np.mean(Y_train)\n",
    "    df.loc['Train', 'RMSE'] = 100 * np.sqrt(np.mean((Y_train - Y_train_pred)**2)) / np.mean(Y_train)\n",
    "    df.loc['Train', 'Bias'] = 100 * np.mean((Y_train - Y_train_pred)) / np.mean(Y_train)\n",
    "\n",
    "    df.loc['Test', 'MAE'] = 100 * np.mean(np.abs(Y_test - Y_test_pred)) / np.mean(Y_test)\n",
    "    df.loc['Test', 'RMSE'] = 100 * np.sqrt(np.mean((Y_test - Y_test_pred)**2)) / np.mean(Y_test)\n",
    "    df.loc['Test', 'Bias'] = 100 * np.mean((Y_test - Y_test_pred)) / np.mean(Y_test)\n",
    "\n",
    "    df = df.astype(float).round(1)  # Round numbers for display\n",
    "    print(df)\n",
    "\n",
    "# Evaluate the model predictions\n",
    "kpi_ML(Y_train, y_train_pred, Y_test, y_test_pred, name='XGBoost_optimized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9380d1",
   "metadata": {},
   "source": [
    "### Multiple Periods Evaluation\n",
    "Just like AdaBoost, XGBoost unfortunately cannot forecast multiple periods at once, so we will also use scikit-learn’s MultiOutputRegressor. To make the model faster, you should set n_jobs=1 in XGBRegressor and n_jobs=-1 for MultiOutputRegressor. (In other words, a thread will be working independently on each of the future forecast periods, rather than multiple threads working on single forecast periods one by one.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8642790",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "# Training and testing\n",
    "X_train, Y_train, X_test, Y_test = datasets(df, x_len=12, y_len=6, test_loops=12)\n",
    "XGB = XGBRegressor(n_jobs=1, **best_params)\n",
    "multi = MultiOutputRegressor(XGB, n_jobs=-1)\n",
    "multi.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the forecast DataFrame with predictions and setting the index\n",
    "forecast = pd.DataFrame(data=multi.predict(X_test), index=df.index)\n",
    "\n",
    "print(forecast.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
